[{"title":"pandas","url":"%2F2019%2F03%2F13%2Fpandas%2F","content":"\n[![思维导图](https://i.loli.net/2019/03/13/5c88814ef04c5.png)](https://i.loli.net/2019/03/13/5c88814ef04c5.png)\n原文件地址：https://mubu.com/doc/3DH__vzNYa\n","tags":["数据分析"]},{"title":"无重复字符的最长子串","url":"%2F2019%2F03%2F12%2F%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2%2F","content":"\n给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。\n\n<!--more-->\n\n**示例 1:**\n\n```\n输入: \"abcabcbb\"\n输出: 3 \n解释: 因为无重复字符的最长子串是 \"abc\"，所以其长度为 3。\n```\n\n**示例 2:**\n\n```\n输入: \"bbbbb\"\n输出: 1\n解释: 因为无重复字符的最长子串是 \"b\"，所以其长度为 1。\n```\n\n**示例 3:**\n\n```\n输入: \"pwwkew\"\n输出: 3\n解释: 因为无重复字符的最长子串是 \"wke\"，所以其长度为 3。\n     请注意，你的答案必须是 子串 的长度，\"pwke\" 是一个子序列，不是子串。\n```\n\n------\n\n1 字典st记录字符出现的位置，两个相同字符之间的距离，就是无重复字符的长度\n\n```\nclass Solution:\n    def lengthOfLongestSubstring(self, s):\n        st = {}\n        i, ans = -1, 0\n        for j in range(len(s)):\n            if s[j] in st:\n                i = max(st[s[j]], i)\n            ans = max(ans, j - i)\n            st[s[j]] = j\n        return ans\n```\n\n2\n\n```\nclass Solution(object):\n    def lengthOfLongestSubstring(self, s):\n        \"\"\"\n        :type s: str\n        :rtype: int\n        \"\"\"\n        maxnum,num,ss =0,0,''\n        for each in s:\n            if each in ss:\n                ss = ss.split(each)[-1]+each\n                num =len(ss)\n            else:\n                num += 1\n                ss += each\n                if num>=maxnum:\n                    maxnum = num\n\n        return maxnum\n```\n\n3\n\n```\nclass Solution:\n    def lengthOfLongestSubstring(self, s):\n        \"\"\"\n        :type s: str\n        :rtype: int\n        \"\"\"\n        res, start, n = 0, 0, len(s)\n        maps = {}\n        for i in range(n):\n            start = max(start, maps.get(s[i], -1)+1)\n            res = max(res, i - start + 1)\n            maps[s[i]] = i\n        return res\n```\n\n4\n\n```\nclass Solution:\n    def lengthOfLongestSubstring(self, s):\n        \"\"\"\n        :type s: str\n        :rtype: int\n        \"\"\"\n        lookup = collections.defaultdict(int)\n        l, r, counter, res = 0, 0, 0, 0\n        while r < len(s):\n            lookup[s[r]] += 1\n            if lookup[s[r]] == 1:\n                counter += 1\n            r += 1\n            # counter < r - l 说明有重复字符出现，正常为counter == r - l\n            while l < r and counter < r - l:\n                lookup[s[l]] -= 1\n                if lookup[s[l]] == 0:\n                    counter -= 1\n                l += 1\n            res = max(res, r - l)\n        return res\n```\n\n","tags":["字符串"]},{"title":"线性回归","url":"%2F2019%2F03%2F12%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F","content":"线性回归本质上是一个函数估计的问题，即找出自变量与因变量的关系。\n - 因变量是连续的，回归\n - 因变量是离散的，分类\n回归分析是一个有监督的学习\n<!--more-->\n## 简单线性回归\n假设某个体x有d个特征，即x=(x^1,x^2,...,x^d)，x^i是第i个特征，线性模型(linear model)试图通过特征的线性组合得到预测值，即\n$$\nf(x)=w^{T}x+b=w_{1}x^{1}+w_{2}x^{2}+...+w_{d}x^{n}+b\n$$\n<!--more-->\n 其中当w_{i}是第个特征的权重，既能调节特征的量纲，也能显示该特征对预测值的重要程度；是第i个特征的权重，既能调节特征的量纲，也能显示该特征对预测值的重要程度\n$$\nw^{T}=（w_{1}，w_{2}，...，w_{d}）\n$$\n\n$$\nx^{T}=（x_{1}，x_{2}，...，x_{d}）\n$$\n\nb代表预测值中非代表预测值中非x所能影响的那部分；当所能影响的那部分；当d=1时，便是最简单的线性模型时，便是最简单的线性模型\n$$\nf(x)=wx+b\n$$\n\n\n线性回归试图学习f(x_i)=wx_i+b，使得f(x_i)≈y_i\n$$\ng(w,b)=\\sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}=\\sum_{i=1}^{n}(wx_{i}+b-y_{i})^{2}\n$$\n让g(w,b)取得最小值。因此我们可以用偏导数求解： \n$$\n\\frac{\\partial g(w,b)}{\\partial w}=0\\\\\n\\frac{\\partial g(w,b)}{\\partial b}=0\n$$\n\n\n&emsp;&emsp;解出：\n\n$$\nw=\\frac{\\sum_{i=1}^{n}y_{i}(x_{i}-\\bar{x})}{\\sum_{i=1}^{n}x_{i}^2-n\\bar{x}^{2}}\\\\\nb=\\bar{y}-w\\bar{x}\\\\\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\\\\n\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}\n$$\n推导过程如下：\n\n- 先求b:\n  $$\n  \\frac{\\partial g(w,b)}{\\partial b}=\\sum_{i=1}^{n}(y_{i}-wx_i-b)(-1)\n  =\\sum_{i=1}^{n}(y_{i}-wx_i-b)=0\n  $$\n  化简得：\n  $$\n  \\sum_{i=1}^{n}y_i-w\\sum_{i=1}^{n}x_i-\\sum_{i=1}^{n}b=\\sum_{i=1}^{n}y_i-w\\sum_{i=1}^{n}x_i-mb=0\n  $$\n  即：\n  $$\n  mb=\\sum_{i=1}^{n}y_i-w\\sum_{i=1}^{n}x_i\n  $$\n  即：\n  $$\n  b=\\bar{y}-w\\bar{x}\n  $$\n\n  - 再求w\n    $$\n    \\frac{\\partial g(w,b)}{\\partial b}=\\sum_{i=1}^{n}2(y_{i}-wx_i-b)(-x_i)=\\sum_{i=1}^{n}(y_{i}-wx_i-b)x_i=0\n    $$\n    代入b：\n    $$\n    \\frac{\\partial g(w,b)}{\\partial b}=\\sum_{i=1}^{n}(y_i-wx_i-\\bar{y}+w\\bar{x})x_i=0\n    $$\n    即：\n    $$\n    \\sum_{i=1}^{n}(x_iy_i-wx_i^2-x_i\\bar{y}+wx_i\\bar{x})=0\n    $$\n    即：\n    $$\n    \\sum_{i=1}^{n}(x_iy_i-x_i\\bar{y})=\\sum_{i=1}^{n}(wx_i^2-wx_i\\bar{x})\n    $$\n    即：\n    $$\n    w=\\frac{\\sum_{i=1}^{n}(x_iy_i-x_i\\bar{y})}{\\sum_{i=1}^{n}(x_i^2-x_i\\bar{x})}\n    $$\n    w的解这样看起来很复杂，可以进一步优化\n\n    其中：\n    $$\n    \\sum_{i=1}^{n}x_i\\bar{y}=\\bar{y}\\sum_{i=1}^{n}x_i=n\\bar{y}\\bar{x}=\\bar{x}\\sum_{i=1}^{n}\\bar{y}=\\sum_{i=1}^{n}\\bar{x}y_i=\\sum_{i=1}^{n}\\bar{x}\\bar{y}\n    $$\n\n\n\n\n\n​             所以：\n$$\nw=\\frac{\\sum_{i=1}^{n}(x_iy_i-x_i\\bar{y}-\\bar{x}y_i+\\bar{x}\\bar{y})}{\\sum_{i=1}^{n}(x_i^2-x_i\\bar{x}-\\bar{x}x_i+\\bar{x})}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\n$$\n\n\n\n## 实现简单线性回归\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.array([1.,2.,3.,4.,5.])\ny = np.array([2.,3.,4.,3.,5.])\nx_mean = np.mean(x)\ny_mean = np.mean(y)\nn = d = 0.0\nfor x_i, y_i in zip(x, y):\n\tn += (x_i-x_mean)*(y_i-y_mean)\n\td += (x_i-x_mean)**2\nw = n/d\nb = y_mean - w*x_mean\ny_hat = a*x + b\n```\n\n### 线性回归的评价指标\n\n均方误差 MSE：$\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2$\n\n均方根误差 RMSE $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}$\n\n平均绝对误差 MAE:  $\\frac{1}{n}\\sum_{i=1}^{n}|y_i-\\hat{y_i}|$\n\nR-squared : $1 - \\frac{MSE(\\hat{y}, y)}{Var(y)}$\n\n## 多元线性回归\n\n目标：使\\sum_{i=1}^{m}(y_i-\\hat{y_i})^2,尽可能的小\n$$\n\\hat{y}^{i}=w_0+w_1{x_1}^{i}+w_2{x_2}^{i}+...+w_n{x_n}^{i}\n$$\n\n即找到${w_0,w_1,w_2,...,w_n}$,使得目标值 尽可能地小\n\n设\n$$\nW = \\mathbf{(w_0,w_1,w_2,...,w_n)}^\\mathrm{T}\n$$\n\n$$\n\\hat{y}^{i}=w_0{x_0}^{i}+w_1{x_1}^{i}+w_2{x_2}^{i}+...+w_n{x_n}^{i},{x_0}^{i}\\equiv1\n$$\n\n$$\nX^{i} = \\mathbf{(X_0^{i},X_1^{i},X_2^{i},...,X_n^{i})}^\\mathrm{T}\n$$\n\n$$\n\\hat{y}^{i}=X^{i}W\n$$\n\n推广：\n$$\nX_b=\\begin{pmatrix}\n        1 & X_1^{(1)} & X_2^{(1)} & \\cdots & X_n^{(1)} \\\\\n        1 &  X_1^{(2)} & X_2^{(2)} & \\cdots & X_n^{(2)} \\\\\n        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        1 &  X_1^{(m)} & X_2^{(m)} & \\cdots & X_n^{(m)} \\\\\n        \\end{pmatrix}\n$$\n$$\nW =\\begin{pmatrix}\n        W_0\\\\W_1\\\\W_2\\\\\\vdots \\\\W_n\n        \\end{pmatrix}\n$$\n$$\n\\hat{y}=X_bW\n$$\n\n使得目标值尽量小，即：\n$$\nJ(W)=\\sum_{i=1}^{m}(y_i-\\hat{y_i})^2=\\mathbf{(Y-X_bW)}^\\mathrm{T}(Y-X_bW)\n$$\n这里还需要用到向量内积对向量求导的知识，对同维度的两个向量U,V,二者向量内积对任意维列向量X求导\n$$\n\\frac{d(\\mathbf{U}^\\mathrm{T}V)}{dx}=\\frac{d(\\mathbf{U}^\\mathrm{T})}{dx}V+\\frac{d(\\mathbf{X}^\\mathrm{T})}{dx}U\n$$\n同理\n$$\n\\frac{d(\\mathbf{X}^\\mathrm{T}X)}{dx}=\\frac{d(\\mathbf{X}^\\mathrm{T})}{dx}X+\\frac{d(\\mathbf{X}^\\mathrm{T})}{dx}X=2IX=2X\n$$\n\n$$\n\\frac{d(\\mathbf{X}^\\mathrm{T}X)}{dy}=\\frac{d(\\mathbf{X}^\\mathrm{T})}{dy}X+\\frac{d(\\mathbf{X}^\\mathrm{T})}{dy}X=2\\frac{d(X^T)}{dy}X\n$$\n\n则：\n$$\n\\frac{\\partial J(W)}{\\partial W}=\\frac{2\\partial\\mathbf{(Y-X_bW)}^\\mathrm{T}}{\\partial W}（Y-X_bW)\n$$\n\n$$\n=2\\frac{\\partial \\mathbf{Y}^\\mathrm{T}}{\\partial W}(Y-X_bW)-2\\frac{\\partial \\mathbf{(X_bW)}^\\mathrm{T}}{\\partial W}(Y-X_bW)\n$$\n\n$$\n=0 - 2\\mathbf{X_b}^{T}(Y-X_bW)\n$$\n\n\n在处理较为复杂的数据的回归问题时，普通的线性回归算法通常会出现预测精度不够，如果模型中的特征之间有相关关系，就会增加模型的复杂程度，并且对整个模型的解释能力并没有提高，这时，就需要对数据中的特征进行选择。\n## 岭回归（Ridge Redression）\n在平方误差的基础上加上正则项\n$$\nJ(θ)=\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}−(wx^{(i)}+b))^2+λ||w||_2^2=MSE(θ)+λ\\sum_{i=1}^{n}θ_i^2\n$$\n通常也写成如下形式：\n$$\nJ(θ)=\\frac{1}{2m}\\sum_{i=1}^{m}(y^{(i)}−(wx^{(i)}+b))^2+\\frac{λ}{2}||w||_2^2=\\frac{1}{2}MSE(θ)+\\frac{λ}{2}\\sum_{i=1}^{n}θ_i^2\n$$\n上式中的w是长度为n的向量，不包括截距项的系数$θ_0$；θ是长度为n+1的向量，包括截距项的系数$θ_0$；m为样本数；n为特征数.\n\n全局最优解为：\n$$\nθ=(\\mathbf{X}^\\mathrm{T}X+λI)^{−1}(\\mathbf{X}^\\mathrm{T}y)\n$$\n\n## Lasso回归\n\n加入L1正则项\n\n代价函数为：\n$$\nJ(θ)=\\frac{1}{2m}\\sum_{i=1}^{m}(y^{(i)}−(wx^{(i)}+b))^2+λ||w||_1=\\frac{1}{2}MSE(θ)+λ\\sum_{i=1}^{n}θ_i\n$$\n上式中的w是长度为n的向量，不包括截距项的系数$θ_0$, θθ是长度为n+1的向量，包括截距项的系数$θ_0$，m为样本数，n为特征数.","tags":["机器学习"]},{"title":"两数相加","url":"%2F2019%2F03%2F11%2F%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0%2F","content":"\n原题链接：https://leetcode-cn.com/problems/add-two-numbers/comments/\n\n给出两个 **非空** 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 **逆序** 的方式存储的，并且它们的每个节点只能存储 **一位** 数字。\n\n如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。\n\n您可以假设除了数字 0 之外，这两个数都不会以 0 开头。\n\n<!--more-->\n\n**示例：**\n\n```\n输入：(2 -> 4 -> 3) + (5 -> 6 -> 4)\n输出：7 -> 0 -> 8\n原因：342 + 465 = 807\n```\n---\n1 依次计算\n```python\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def addTwoNumbers(self, l1, l2):\n        \"\"\"\n        :type l1: ListNode\n        :type l2: ListNode\n        :rtype: ListNode\n        \"\"\"\n        add = 0\n        l3 = ListNode(0)\n        node = l3\n        while l1 or l2:\n            cur = ListNode(add)\n            if l1:\n                cur.val += l1.val\n                l1 = l1.next\n            if l2:\n                cur.val += l2.val\n                l2 = l2.next\n            add = cur.val // 10\n            cur.val = cur.val % 10\n            node.next, node = cur, cur\n        if add:\n            node.next = ListNode(add)\n        \n        return l3.next\n```\n2 全部变成数字，然后再转换回来\n```python\nclass Solution:\n    def addTwoNumbers(self, l1, l2):\n        \"\"\"\n        :type l1: ListNode\n        :type l2: ListNode\n        :rtype: ListNode\n        \"\"\"\n        if not l1:\n            return l2\n        if not l2:\n            return l1\n        \n        val1, val2 = [l1.val], [l2.val]\n        while l1.next:\n            val1.append(l1.next.val)\n            l1 = l1.next\n        while l2.next:\n            val2.append(l2.next.val)\n            l2 = l2.next\n            \n        num1 = ''.join([str(i) for i in val1[::-1]])\n        num2 = ''.join([str(i) for i in val2[::-1]])\n        \n        tmp = str(int(num1) + int(num2))[::-1]\n        res = ListNode(int(tmp[0]))\n        run_res = res\n        for i in range(1, len(tmp)):\n            run_res.next = ListNode(int(tmp[i]))\n            run_res = run_res.next\n        return res\n```\n3 使用递归 \n```python\nclass Solution:\n    def addTwoNumbers(self, l1, l2):\n        \"\"\"\n        :type l1: ListNode\n        :type l2: ListNode\n        :rtype: ListNode\n        \"\"\"\n        if not l1 and not l2:\n            return \n        elif not (l1 and l2):\n            return l1 or l2\n        else:\n            if l1.val + l2.val < 10:\n                l3 = ListNode(l1.val+l2.val)\n                l3.next = self.addTwoNumbers(l1.next, l2.next)\n            else:\n                l3 = ListNode(l1.val+l2.val-10)\n                l3.next = self.addTwoNumbers(l1.next, self.addTwoNumbers(l2.next, ListNode(1)))\n        return l3\n```","tags":["递归"]},{"title":"机器人的运动范围","url":"%2F2019%2F03%2F11%2F%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%BF%90%E5%8A%A8%E8%8C%83%E5%9B%B4%2F","content":"\n地上有一个 m行和 n列的方格，横纵坐标范围分别是 0∼m−10∼m−1 和 0∼n−10∼n−1。\n\n一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格。\n\n但是不能进入行坐标和列坐标的数位之和大于 kk 的格子。\n\n请问该机器人能够达到多少个格子？\n\n<!--more-->\n\n#### 样例1\n\n```\n输入：k=7, m=4, n=5\n\n输出：20\n```\n\n#### 样例2\n\n```\n输入：k=18, m=40, n=40\n\n输出：1484\n\n解释：当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。\n      但是，它不能进入方格（35,38），因为3+5+3+8 = 19。\n```\n\n**注意**:\n\n1. 0<=m<=50\n2. 0<=n<=50\n3. 0<=k<=100\n\n```python\nclass Solution:\n    def movingCount(self, threshold, rows, cols):\n        cnt = 0\n        visited = [[False]*cols for _  in range(rows)]\n         \n        def dfs(x, y):\n            if not (0<=x<rows) or not (0<=y<cols) or  visited[x][y] or sum(map(int,str(x)+str(y))) > threshold:\n                return 0\n             \n            visited[x][y] = True\n            return 1 +  dfs(x+1,y) +  dfs(x-1,y) + dfs(x,y+1) + dfs(x,y-1)         \n        return dfs(0,0)\n```\n\n```c++\nclass Solution {\npublic:\n    int movingCount(int threshold, int rows, int cols)\n    {\n       bool *flag = new bool[rows * cols];\n        for(int i = 0; i < rows * cols; i++)\n            flag[i] = false;\n        int count = moving(threshold, rows, cols, 0, 0, flag);\n        return count; \n    }\n     int moving(int threshold, int rows, int cols, int i, int j, bool* flag)\n        {\n        int count = 0;\n        if(i >= 0 && i < rows && j >= 0 && j < cols && getsum(i) + getsum(j) <= threshold && flag[i * cols + j] == false)\n            {\n            flag[i * cols + j] = true;\n            count =1 + moving(threshold, rows, cols, i + 1, j, flag)\n                + moving(threshold, rows, cols, i - 1, j, flag)\n                + moving(threshold, rows, cols, i , j - 1, flag)\n                + moving(threshold, rows, cols, i, j + 1, flag);\n        }\n        return count;\n    }\n    int getsum(int num)\n        {\n        int sum = 0;\n        while(num)\n            {\n            sum += num % 10;\n            num /= 10;\n              \n        }\n        return sum;\n    }\n};\n```\n\n\n\n## 思路\n\n**1.从(0,0)开始走，每成功走一步标记当前位置为true,然后从当前位置往四个方向探索，**\n\n**返回1 + 4 个方向的探索值之和。**\n\n**2.探索时，判断当前节点是否可达的标准为：**\n\n  1）当前节点在矩阵内\n\n  2）当前节点未被访问过\n\n  3）当前节点满足limit限制。","tags":["回溯"]},{"title":"杂念","url":"%2F2019%2F03%2F10%2F%E6%9D%82%E5%BF%B5%2F","content":"\n算了，本来想提笔写些什么的，但是写不出来，哪怕只是说与自己听。\n\n","tags":["随笔"]},{"title":"excel数据分析函数","url":"%2F2019%2F03%2F10%2Fexcel%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0%2F","content":"\n## 自动插入排名\n\n```\nRANK.EQ(L16,$L$16:$L$25)\n```\n\n- RANK.EQ 函数：返回某数字在一列数字中相对于其他数值的大小排名；如果多个数值排名相同，则返回该组数值的最佳排名\n\n  　<!-- more -->　\n\n##  统计前3球队的平均积分\n\n```\nAVERAGE(LARGE($L$16:$L$25,{1;2;3}))\n```\n\n- LARGE 函数：返回数据集中第 k 个或前 k 个最大值\n- AVERAGE 函数：返回其参数的平均值\n\n# 线性拟合\n源数据\n\n|  7   | 12600 |\n| :--: | :---: |\n|  8   | 15600 |\n|  9   | 9700  |\n|  10  | 11360 |\n|  11  | 15540 |\n|  12  | 17480 |\n\n\n\n\n## LINEST\n\n![函数图例](https://i.loli.net/2019/03/10/5c84e66d101e8.png)\n\n参数说明：\n\n- Known_y's是目标值y,\n- Known_x's是自变量x\n-  stats 可选，是否返回附加回归统计值 \n- const 可选，是否将常量强制设置为0\n### 获取斜率和截距\nINDEX\n```\n=INDEX(LINEST(B2:B7,A2:A7),1)  #斜率\n=INDEX(LINEST(B2:B7,A2:A7),2)  #截距\n```\n## TREND\n返回预测的线性回归的值 \n```\n=TREND(B2:B7,A2:A7,A33:A38)\n```\n## LOGEST\n描述的曲线是：y=b*m^x\n```\n=LOGEST(B2:B7,A2:A7,,1)\n```\n### 获取系数\n```\n=INDEX(LOGEST(B2:B7,A2:A7),1)  # m\n=INDEX(LOGEST(B2:B7,A2:A7),2)  # b\n```\n## GROWTH\n\n返回要预测的值 \n\n```\n=GROWTH(B2:B7,A2:A7,A33:A38)\n```","tags":["数据分析"]},{"title":"翻转二叉树","url":"%2F2019%2F03%2F10%2F%E7%BF%BB%E8%BD%AC%E4%BA%8C%E5%8F%89%E6%A0%91%2F","content":"\n翻转一棵二叉树。\n\n<!--more-->\n\n示例：\n\n\n```\n输入：\n\n     4\n   /   \\\n  2     7\n / \\   / \\\n1   3 6   9\n输出：\n\n     4\n   /   \\\n  7     2\n / \\   / \\\n9   6 3   1\n```\n\n---\n\n```\n# Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.left = None\n#         self.right = None\n\nclass Solution:\n    def invertTree(self, root):\n        \"\"\"\n        :type root: TreeNode\n        :rtype: TreeNode\n        \"\"\"\n        if root==None:\n            return root\n        root.left, root.right = root.right, root.left\n        self.invertTree(root.left)\n        self.invertTree(root.right)\n        return root\n```","tags":["二叉树"]}]